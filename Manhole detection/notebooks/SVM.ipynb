{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCRV2LH9B1mUR61v+uPpEC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d8aSfrfzkBx3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696761330126,"user_tz":-330,"elapsed":21179,"user":{"displayName":"S Vijay Sai Kumar","userId":"07982524899240864297"}},"outputId":"0fe2f776-37fa-4e35-98f4-0478a0fd2754"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import glob\n","from skimage.transform import resize\n","from skimage.feature import hog\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","import seaborn as sns"],"metadata":{"id":"ePSeMb52k0qd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load and preprocess the data\n","def load_data(manhole, tipo):\n","    label = []\n","    arr = []\n","    strr = \"/content/drive/MyDrive/Manhole detection/Man_hole_DB/\" + manhole + \"/\" + tipo + \"/*\"\n","    for file_ in glob.glob(strr):\n","        img = cv2.imread(file_)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        arr.append(img)\n","        label.append(manhole)\n","    return arr, label"],"metadata":{"id":"rxPcfg5HlENT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def whole_train_data(tipo):\n","    closed_data, closed_label = load_data('Closed', tipo)\n","    open_data, open_label = load_data('Open', tipo)\n","    road_data, road_label = load_data('Road', tipo)\n","    data = np.concatenate((closed_data, open_data, road_data))\n","    labels = np.concatenate((closed_label, open_label, road_label))\n","    return data, labels"],"metadata":{"id":"UcNb05rIlrsJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_train, labels_train = whole_train_data('Train')\n","data_test, labels_test = whole_train_data('Test')\n","data_train.shape, labels_train.shape"],"metadata":{"id":"KgeQS-Belx9F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696761440433,"user_tz":-330,"elapsed":106434,"user":{"displayName":"S Vijay Sai Kumar","userId":"07982524899240864297"}},"outputId":"cacafae2-cb25-4a05-9456-7ad0e1a0ac49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"execute_result","data":{"text/plain":["((3715,), (3715,))"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Function to preprocess the images for HOG feature extraction\n","def preprocessing_part_two(arr):\n","    arr_feature = []\n","    for i in range(np.shape(arr)[0]):\n","        img = arr[i]\n","        # Resize the image to (72, 72)\n","        img_resized = resize(img, (72, 72), anti_aliasing=True)\n","        arr_feature.append(img_resized)\n","    return arr_feature"],"metadata":{"id":"qzWbmuW1lxcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_train_p = np.array(preprocessing_part_two(data_train))\n","data_test_p = np.array(preprocessing_part_two(data_test))"],"metadata":{"id":"8M0QFq2Wl7Id"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ExtractHOG(img):\n","    if img.ndim == 3:  # If the image is color (3D), convert it to grayscale\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    ftr, _ = hog(img, orientations=8, pixels_per_cell=(16, 16),\n","                 cells_per_block=(1, 1), visualize=True, multichannel=False)\n","    return ftr\n"],"metadata":{"id":"HpxXj-SHl-S3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_train_ftr = [ExtractHOG(img) for img in data_train_p]\n","data_test_ftr = [ExtractHOG(img) for img in data_test_p]\n"],"metadata":{"id":"2YbpV4lXl-W0","colab":{"base_uri":"https://localhost:8080/","height":459},"executionInfo":{"status":"error","timestamp":1696761519661,"user_tz":-330,"elapsed":18,"user":{"displayName":"S Vijay Sai Kumar","userId":"07982524899240864297"}},"outputId":"48d31ee6-d0b5-4e8f-a547-d0a788c79577"},"execution_count":null,"outputs":[{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-c8d690c5bcd0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_train_ftr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mExtractHOG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_train_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_test_ftr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mExtractHOG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_test_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-c8d690c5bcd0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_train_ftr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mExtractHOG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_train_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_test_ftr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mExtractHOG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_test_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-e2f5a6c33718>\u001b[0m in \u001b[0;36mExtractHOG\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mExtractHOG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# If the image is color (3D), convert it to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     ftr, _ = hog(img, orientations=8, pixels_per_cell=(16, 16),\n\u001b[1;32m      5\u001b[0m                  cells_per_block=(1, 1), visualize=True, multichannel=False)\n","\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<1>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n"]}]},{"cell_type":"code","source":["# Train the SVM classifier\n","svm_clf = SVC(kernel='linear')\n","svm_clf.fit(data_train_ftr, labels_train)"],"metadata":{"id":"DoIKfZOQl-ao"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make predictions and calculate accuracy and F1 score\n","y_pred = svm_clf.predict(data_test_ftr)\n","accuracy = accuracy_score(labels_test, y_pred)\n","f1 = f1_score(labels_test, y_pred, average='weighted')\n","print('Test Accuracy:', accuracy * 100, \"%\")\n","print('F1 Score:', f1)"],"metadata":{"id":"gSG5cc4vl-dz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate the confusion matrix\n","cm = confusion_matrix(labels_test, y_pred)"],"metadata":{"id":"ENnPDBLMl-xU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the class names for the labels\n","class_names = np.unique(labels_train)"],"metadata":{"id":"EFps8KAvzbyK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the confusion matrix as a heatmap\n","plt.figure(figsize=(6, 4))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","plt.show()"],"metadata":{"id":"kdPvytsJxGtV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display random test image and its predicted label\n","def showImg(img, name):\n","    plt.axis(\"off\")\n","    plt.title(name)\n","    plt.imshow(img)\n","    plt.show()"],"metadata":{"id":"402Vx9_F0hm8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"92c0E_xcxOtb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import glob\n","from skimage.transform import resize\n","from skimage.feature import hog\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","import seaborn as sns\n","# Load and preprocess the data\n","def load_data(manhole, tipo):\n","    label = []\n","    arr = []\n","    strr = \"/content/drive/MyDrive/Manhole detection/Man_hole_DB/\" + manhole + \"/\" + tipo + \"/*\"\n","    for file_ in glob.glob(strr):\n","        img = cv2.imread(file_)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        arr.append(img)\n","        label.append(manhole)\n","    return arr, label\n","def whole_train_data(tipo):\n","    closed_data, closed_label = load_data('Closed', tipo)\n","    open_data, open_label = load_data('Open', tipo)\n","    road_data, road_label = load_data('Road', tipo)\n","    data = np.concatenate((closed_data, open_data, road_data))\n","    labels = np.concatenate((closed_label, open_label, road_label))\n","    return data, labels\n","data_train, labels_train = whole_train_data('Train')\n","data_test, labels_test = whole_train_data('Test')\n","data_train.shape, labels_train.shape\n","# Function to preprocess the images for HOG feature extraction\n","def preprocessing_part_two(arr):\n","    arr_feature = []\n","    for i in range(np.shape(arr)[0]):\n","        img = arr[i]\n","        # Resize the image to (72, 72)\n","        img_resized = resize(img, (72, 72), anti_aliasing=True)\n","        arr_feature.append(img_resized)\n","    return arr_feature\n","data_train_p = np.array(preprocessing_part_two(data_train))\n","data_test_p = np.array(preprocessing_part_two(data_test))\n","def ExtractHOG(img):\n","    if img.ndim == 3:  # If the image is color (3D), convert it to grayscale\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    ftr, _ = hog(img, orientations=8, pixels_per_cell=(16, 16),\n","                 cells_per_block=(1, 1), visualize=True, multichannel=False)\n","    return ftr\n","data_train_ftr = [ExtractHOG(img) for img in data_train_p]\n","data_test_ftr = [ExtractHOG(img) for img in data_test_p]\n","# Train the SVM classifier\n","svm_clf = SVC(kernel='linear')\n","svm_clf.fit(data_train_ftr, labels_train)\n","# Make predictions and calculate accuracy and F1 score\n","y_pred = svm_clf.predict(data_test_ftr)\n","accuracy = accuracy_score(labels_test, y_pred)\n","f1 = f1_score(labels_test, y_pred, average='weighted')\n","print('Test Accuracy:', accuracy * 100, \"%\")\n","print('F1 Score:', f1)\n","# Generate the confusion matrix\n","cm = confusion_matrix(labels_test, y_pred)\n","# Define the class names for the labels\n","class_names = np.unique(labels_train)\n","# Plot the confusion matrix as a heatmap\n","plt.figure(figsize=(6, 4))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","plt.show()\n","# Display random test image and its predicted label\n","def showImg(img, name):\n","    plt.axis(\"off\")\n","    plt.title(name)\n","    plt.imshow(img)\n","    plt.show()\n","from random import seed\n","from random import randint\n","x_ = randint(0, data_test_p.shape[0] - 1)\n","predicted_label = svm_clf.predict([data_test_ftr[x_]])\n","predicted_manhole = predicted_label[0]\n","showImg(data_test[x_], predicted_manhole)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":566},"id":"a869BM3_MOZe","executionInfo":{"status":"error","timestamp":1696767399161,"user_tz":-330,"elapsed":210268,"user":{"displayName":"S Vijay Sai Kumar","userId":"07982524899240864297"}},"outputId":"b6bdad94-a12c-4b3c-d255-9a4e78c8df8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-deb118d63549>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m                  cells_per_block=(1, 1), visualize=True, multichannel=False)\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mftr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mdata_train_ftr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mExtractHOG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_train_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mdata_test_ftr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mExtractHOG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_test_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Train the SVM classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-deb118d63549>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m                  cells_per_block=(1, 1), visualize=True, multichannel=False)\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mftr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mdata_train_ftr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mExtractHOG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_train_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mdata_test_ftr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mExtractHOG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_test_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Train the SVM classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-deb118d63549>\u001b[0m in \u001b[0;36mExtractHOG\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mExtractHOG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# If the image is color (3D), convert it to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     ftr, _ = hog(img, orientations=8, pixels_per_cell=(16, 16),\n\u001b[1;32m     48\u001b[0m                  cells_per_block=(1, 1), visualize=True, multichannel=False)\n","\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<1>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n"]}]}]}